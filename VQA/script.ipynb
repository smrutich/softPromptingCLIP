{"cells":[{"cell_type":"markdown","metadata":{"id":"X0_tIyaQnmoR"},"source":["# Installs, Imports, Preps"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9_Mdk6Ie-Tbj"},"outputs":[],"source":["# !pip install git+https://github.com/openai/CLIP.git\n","# !pip install transformers\n","# !pip install yacs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CvAchq0Ea0wk"},"outputs":[],"source":["import clip\n","import torch\n","import transformers\n","import torchvision\n","import gc\n","import os\n","import torch.nn as nn\n","import torch.optim as optim\n","# import torch.nn.functional as F\n","from torch.utils.data import DataLoader, Subset\n","from tqdm import tqdm\n","from config import get_cfg_defaults\n","from utils import build_lr_scheduler\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","cfg = get_cfg_defaults()\n","\n","if not os.path.exists('checkpoints'):\n","    os.mkdir('checkpoints')"]},{"cell_type":"markdown","metadata":{"id":"TNBHu4fBntAv"},"source":["# Image Encoding"]},{"cell_type":"markdown","metadata":{"id":"tkmJqC_BVPRN"},"source":["## Load data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"noI9JFemVPRN","outputId":"29b8ac41-9d3a-45f8-9956-31289b8e9461"},"outputs":[{"data":{"text/plain":["CLIP(\n","  (visual): VisionTransformer(\n","    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n","    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    (transformer): Transformer(\n","      (resblocks): Sequential(\n","        (0): ResidualAttentionBlock(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","          )\n","          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (mlp): Sequential(\n","            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n","            (gelu): QuickGELU()\n","            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (1): ResidualAttentionBlock(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","          )\n","          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (mlp): Sequential(\n","            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n","            (gelu): QuickGELU()\n","            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (2): ResidualAttentionBlock(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","          )\n","          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (mlp): Sequential(\n","            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n","            (gelu): QuickGELU()\n","            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (3): ResidualAttentionBlock(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","          )\n","          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (mlp): Sequential(\n","            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n","            (gelu): QuickGELU()\n","            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (4): ResidualAttentionBlock(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","          )\n","          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (mlp): Sequential(\n","            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n","            (gelu): QuickGELU()\n","            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (5): ResidualAttentionBlock(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","          )\n","          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (mlp): Sequential(\n","            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n","            (gelu): QuickGELU()\n","            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (6): ResidualAttentionBlock(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","          )\n","          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (mlp): Sequential(\n","            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n","            (gelu): QuickGELU()\n","            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (7): ResidualAttentionBlock(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","          )\n","          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (mlp): Sequential(\n","            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n","            (gelu): QuickGELU()\n","            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (8): ResidualAttentionBlock(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","          )\n","          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (mlp): Sequential(\n","            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n","            (gelu): QuickGELU()\n","            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (9): ResidualAttentionBlock(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","          )\n","          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (mlp): Sequential(\n","            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n","            (gelu): QuickGELU()\n","            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (10): ResidualAttentionBlock(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","          )\n","          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (mlp): Sequential(\n","            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n","            (gelu): QuickGELU()\n","            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (11): ResidualAttentionBlock(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","          )\n","          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (mlp): Sequential(\n","            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n","            (gelu): QuickGELU()\n","            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","    )\n","    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (transformer): Transformer(\n","    (resblocks): Sequential(\n","      (0): ResidualAttentionBlock(\n","        (attn): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","        )\n","        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Sequential(\n","          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n","          (gelu): QuickGELU()\n","          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n","        )\n","        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (1): ResidualAttentionBlock(\n","        (attn): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","        )\n","        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Sequential(\n","          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n","          (gelu): QuickGELU()\n","          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n","        )\n","        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (2): ResidualAttentionBlock(\n","        (attn): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","        )\n","        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Sequential(\n","          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n","          (gelu): QuickGELU()\n","          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n","        )\n","        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (3): ResidualAttentionBlock(\n","        (attn): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","        )\n","        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Sequential(\n","          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n","          (gelu): QuickGELU()\n","          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n","        )\n","        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (4): ResidualAttentionBlock(\n","        (attn): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","        )\n","        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Sequential(\n","          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n","          (gelu): QuickGELU()\n","          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n","        )\n","        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (5): ResidualAttentionBlock(\n","        (attn): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","        )\n","        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Sequential(\n","          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n","          (gelu): QuickGELU()\n","          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n","        )\n","        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (6): ResidualAttentionBlock(\n","        (attn): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","        )\n","        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Sequential(\n","          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n","          (gelu): QuickGELU()\n","          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n","        )\n","        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (7): ResidualAttentionBlock(\n","        (attn): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","        )\n","        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Sequential(\n","          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n","          (gelu): QuickGELU()\n","          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n","        )\n","        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (8): ResidualAttentionBlock(\n","        (attn): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","        )\n","        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Sequential(\n","          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n","          (gelu): QuickGELU()\n","          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n","        )\n","        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (9): ResidualAttentionBlock(\n","        (attn): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","        )\n","        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Sequential(\n","          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n","          (gelu): QuickGELU()\n","          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n","        )\n","        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (10): ResidualAttentionBlock(\n","        (attn): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","        )\n","        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Sequential(\n","          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n","          (gelu): QuickGELU()\n","          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n","        )\n","        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (11): ResidualAttentionBlock(\n","        (attn): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","        )\n","        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Sequential(\n","          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n","          (gelu): QuickGELU()\n","          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n","        )\n","        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","    )\n","  )\n","  (token_embedding): Embedding(49408, 512)\n","  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",")"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["model, transform = clip.load(cfg.MODEL.NAME)\n","model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BEdhyyF7cr-H"},"outputs":[],"source":["# If download is necessary, use this\n","# dataset = torchvision.datasets.Flowers102(root='',transform=transform,download=True,split='train')\n","\n","# If already downloaded, use this\n","dataset = torchvision.datasets.Flowers102(root='',transform=transform,split='train')"]},{"cell_type":"markdown","metadata":{"id":"j11ycRkTVPRO"},"source":["I keep running into OOM error, so I have to subset the dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IcIf7_bnVPRO"},"outputs":[],"source":["subsets = [Subset(dataset, range(i,i+204)) for i in range(0, len(dataset), len(dataset)//5)]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R1rjAQdFVPRO"},"outputs":[],"source":["dataloaders = [DataLoader(subset, batch_size=cfg.DATASET.IMG_BATCH_SIZE, shuffle=False) for subset in subsets]"]},{"cell_type":"markdown","metadata":{"id":"l_NsnS9QsBJG"},"source":["## Get image embs"]},{"cell_type":"markdown","metadata":{"id":"1DW0YOkUsGjV"},"source":["Confirm image shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XJYtz88PsIez","outputId":"175774e8-a108-449e-eddd-963f07f712c2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Image shape: torch.Size([3, 224, 224])\n","Label type:  <class 'int'>\n"]}],"source":["print(f\"Image shape: {dataset[0][0].shape}\")\n","print(f\"Label type:  {type(dataset[0][1])}\")"]},{"cell_type":"markdown","metadata":{"id":"aXjzAadZslhY"},"source":["Encode"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zuV0GaxRVPRP"},"outputs":[],"source":["def free_gpu_cache():\n","    gc.collect()\n","    torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ykYO9EouVPRP","outputId":"8f51299d-5719-44fd-ff03-f1c94e6380cf"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 13/13 [00:01<00:00,  9.95it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Image embedding shape: torch.Size([204, 512])\n","Labels shape:          torch.Size([204])\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 13/13 [00:01<00:00,  9.69it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Image embedding shape: torch.Size([204, 512])\n","Labels shape:          torch.Size([204])\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 13/13 [00:01<00:00,  9.80it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Image embedding shape: torch.Size([204, 512])\n","Labels shape:          torch.Size([204])\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 13/13 [00:01<00:00, 10.15it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Image embedding shape: torch.Size([204, 512])\n","Labels shape:          torch.Size([204])\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 13/13 [00:01<00:00, 10.26it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Image embedding shape: torch.Size([204, 512])\n","Labels shape:          torch.Size([204])\n"]}],"source":["for i,dataloader in enumerate(dataloaders):\n","    img_embs, lbls = [], []\n","    for images, labels in tqdm(dataloader):\n","        img_embs.append(model.encode_image(images.to(device)))\n","        lbls.append(labels)\n","    free_gpu_cache()\n","    img_embs, lbls = torch.vstack(img_embs), torch.hstack(lbls)\n","    print(f\"Image embedding shape: {img_embs.shape}\")\n","    print(f\"Labels shape:          {lbls.shape}\")\n","    torch.save(img_embs, f'checkpoints/{cfg.DATASET.NAME}_image_embs_{i}.pt')\n","    torch.save(lbls,     f'checkpoints/{cfg.DATASET.NAME}_labels_{i}.pt')"]},{"cell_type":"markdown","metadata":{"id":"BOWSthETVPRQ"},"source":["Aggregate temp files"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rn-eqhJaVPRQ"},"outputs":[],"source":["img_embs = torch.vstack([torch.load(f'checkpoints/{cfg.DATASET.NAME}_image_embs_{i}.pt') for i in range(len(dataloaders))])\n","lbls = torch.hstack([torch.load(f'checkpoints/{cfg.DATASET.NAME}_labels_{i}.pt') for i in range(len(dataloaders))])"]},{"cell_type":"markdown","metadata":{"id":"GYEBp5AyvHCY"},"source":["Save for future use"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vC4SSDG2vJgC"},"outputs":[],"source":["torch.save(img_embs, f'checkpoints/{cfg.DATASET.NAME}_image_embs.pt')\n","torch.save(lbls,     f'checkpoints/{cfg.DATASET.NAME}_labels.pt')"]},{"cell_type":"markdown","metadata":{"id":"YrddUuwhweDj"},"source":["# Prompt Encoding"]},{"cell_type":"markdown","metadata":{"id":"cFpyXOMZVPRQ"},"source":["## Prompt Learner"]},{"cell_type":"markdown","metadata":{"id":"EJaJs2UZVPRQ"},"source":["### CoOp"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zT4StczkzJQD"},"outputs":[],"source":["class CoOp(nn.Module):\n","    def __init__(self, cfg, model):\n","        super().__init__()\n","        self.n_cls = len(cfg.DATASET.CLASSNAMES)\n","        self.n_ctx = cfg.TRAIN.N_CTX\n","        ctx_dim = model.ln_final.weight.shape[0]\n","        \n","        # A prompt in CoOp with classname at the back looks like: [SOS][V1]...[Vn][CLS][EOS]\n","        # I will assume classname is always at the back for now.\n","        \n","        # context init\n","        if cfg.TRAIN.CTX_INIT:\n","            # fixed init (assume global ctx)\n","            ctx_init = cfg.TRAIN.CTX_INIT.replace(\"_\", \" \") # The \"_\" is for fill-in of class name\n","            with torch.no_grad():\n","                token_emb = model.token_embedding(clip.tokenize(ctx_init)).type(model.dtype)\n","            ctx_vectors = token_emb[0, 1:self.n_ctx+1, :]\n","            prefix = ctx_init\n","        else:\n","            # random init\n","            if cfg.TRAIN.CSC:\n","                # class-specific ctx\n","                ctx_vectors = torch.empty(self.n_cls, self.n_ctx, ctx_dim, dtype=model.dtype)\n","            else:\n","                # global ctx\n","                ctx_vectors = torch.empty(self.n_ctx, ctx_dim, dtype=model.dtype)\n","            nn.init.normal_(ctx_vectors, std=cfg.TRAIN.PARAM_STD)\n","            prefix = \" \".join([\"X\"]*self.n_ctx)\n","        \n","        # context vectors (THE ONLY PART THAT NEEDS TO BE TRAINED)\n","        self.ctx = nn.Parameter(ctx_vectors)\n","        \n","        # prompt finalization\n","        classnames = [classname.replace(\"_\", \" \") for classname in cfg.DATASET.CLASSNAMES]\n","        raw_prompts = [prefix + \" \" + classname + \".\" for classname in classnames]\n","        self.tokenized_prompts = torch.cat([clip.tokenize(raw_prompt) for raw_prompt in raw_prompts])\n","        with torch.no_grad():\n","            token_emb = model.token_embedding(self.tokenized_prompts).type(model.dtype)\n","        \n","        # [SOS]\n","        self.register_buffer(\"prefix\", token_emb[:, :1, :])\n","        # [CLS][EOS]\n","        self.register_buffer(\"suffix\", token_emb[:, self.n_ctx+1:, :])\n","        \n","    def forward(self):\n","        # expand global ctx to match n_cls (i.e., a total of n_cls ctx vectors)\n","        ctx = self.ctx\n","        if ctx.dim() == 2:\n","            ctx = ctx.unsqueeze(0).expand(self.n_cls, -1, -1)\n","\n","        prompt_embs = torch.cat(\n","            [\n","                self.prefix,    # (n_cls, 1, dim)\n","                ctx,            # (n_cls, n_ctx, dim)\n","                self.suffix     # (n_cls, sfx_len, dim)\n","            ], dim=1\n","        )\n","        \n","        return prompt_embs"]},{"cell_type":"markdown","metadata":{"id":"R0Y6iKC7VPRR"},"source":["### CoCoOp"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zE7AruwvVPRR"},"outputs":[],"source":["class CoCoOp(nn.Module):\n","    def __init__(self, cfg, model):\n","        super().__init__()\n","        self.n_cls = len(cfg.DATASET.CLASSNAMES)\n","        self.n_ctx = cfg.TRAIN.N_CTX\n","        self.dtype = model.dtype\n","        ctx_dim = model.ln_final.weight.shape[0]\n","        vis_dim = model.visual.output_dim\n","        # img_dim = model.visual.input_resolution\n","        \n","        # A prompt in CoOp with classname at the back looks like: [SOS][V1]...[Vn][CLS][EOS]\n","        # I will assume classname is always at the back for now.\n","        \n","        # context init (always global in cocoop)\n","        if cfg.TRAIN.CTX_INIT:\n","            # fixed init\n","            ctx_init = cfg.TRAIN.CTX_INIT.replace(\"_\", \" \") # The \"_\" is for fill-in of class name\n","            with torch.no_grad():\n","                token_emb = model.token_embedding(clip.tokenize(ctx_init)).type(model.dtype)\n","            ctx_vectors = token_emb[0, 1:self.n_ctx+1, :]\n","            prefix = ctx_init\n","        else:\n","            # random init\n","            ctx_vectors = torch.empty(self.n_ctx, ctx_dim, dtype=model.dtype)\n","            nn.init.normal_(ctx_vectors, std=cfg.TRAIN.PARAM_STD)\n","            prefix = \" \".join([\"X\"]*self.n_ctx)\n","        \n","        # context vectors\n","        self.ctx = nn.Parameter(ctx_vectors)      \n","          \n","        # FF (image -> ctx bias)\n","        self.net = nn.Sequential(\n","            nn.Linear(vis_dim, vis_dim//16),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(vis_dim//16, ctx_dim)\n","        )\n","        \n","        # prompt finalization\n","        classnames = [classname.replace(\"_\", \" \") for classname in cfg.DATASET.CLASSNAMES]\n","        raw_prompts = [prefix + \" \" + classname + \".\" for classname in classnames]\n","        self.tokenized_prompts = torch.cat([clip.tokenize(raw_prompt) for raw_prompt in raw_prompts])\n","        with torch.no_grad():\n","            token_emb = model.token_embedding(self.tokenized_prompts).type(model.dtype)\n","        \n","        # [SOS]\n","        self.register_buffer(\"prefix\", token_emb[:, :1, :])\n","        # [CLS][EOS]\n","        self.register_buffer(\"suffix\", token_emb[:, self.n_ctx+1:, :])\n","        \n","    def forward(self, img_feats):\n","        bias = self.net(img_feats.type(self.dtype)).unsqueeze(1) # (batch, 1, dim)\n","        ctx = self.ctx\n","        ctx = ctx.unsqueeze(0)                  # (1, n_ctx, dim)\n","        ctx_shifted = ctx + bias                # (batch, n_ctx, dim)\n","        \n","        prompt_embs = []\n","        for ctx_shifted_i in ctx_shifted:\n","            ctx_i = ctx_shifted_i.unsqueeze(0).expand(self.n_cls, -1, -1)\n","\n","            prompt_emb = torch.cat(\n","                [\n","                    self.prefix,    # (n_cls, 1, dim)\n","                    ctx_i,          # (n_cls, n_ctx, dim)\n","                    self.suffix     # (n_cls, sfx_len, dim)\n","                ], dim=1\n","            )\n","            \n","            prompt_embs.append(prompt_emb)\n","        \n","        return torch.stack(prompt_embs)"]},{"cell_type":"markdown","metadata":{"id":"Xk9O_eiVVPRS"},"source":["## Custom Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zf10qVQkVPRS"},"outputs":[],"source":["class CustomCLIPCoOp(nn.Module):\n","    def __init__(self, cfg, model):\n","        super().__init__()\n","        self.prompt_learner = CoOp(cfg, model)\n","        # self.classnames = cfg.DATASET.CLASSNAMES\n","        self.model = model\n","        \n","        # The freezing part was originally done in the training part, but why not just here since we are not modifying anything of CLIP anyway?\n","        for _,param in self.model.named_parameters():\n","            param.requires_grad = False\n","    \n","    # note that this is nearly identical to the model.encode_text() function from CLIP\n","    # the only difference is that we already have prompt_embs rather than having to recompute it\n","    def encode_text(self, prompt_embs, tokenized_prompts):\n","        x = prompt_embs + self.model.positional_embedding.type(self.model.dtype)\n","        x = x.permute(1, 0, 2)  # NLD -> LND\n","        x = self.model.transformer(x)\n","        x = x.permute(1, 0, 2)  # LND -> NLD\n","        x = self.model.ln_final(x).type(self.model.dtype)\n","\n","        # x.shape = [batch_size, n_ctx, transformer.width]\n","        # take features from the eot embedding (eot_token is the highest number in each sequence)\n","        x = x[torch.arange(x.shape[0]), tokenized_prompts.argmax(dim=-1)] @ self.model.text_projection\n","        return x\n","    \n","    def forward(self, img_feats):\n","        prompt_embs = self.prompt_learner()\n","\n","        # encode prompts\n","        txt_feats = self.encode_text(prompt_embs, self.prompt_learner.tokenized_prompts)\n","        \n","        # normalize\n","        img_feats = img_feats/img_feats.norm(dim=-1, keepdim=True).type(self.model.dtype)   # It was float16 but model.dtype = float32.\n","        txt_feats = txt_feats/txt_feats.norm(dim=-1, keepdim=True)\n","        \n","        logits = self.model.logit_scale.exp() * img_feats @ txt_feats.t()\n","        return logits"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CGASBC5OVPRS"},"outputs":[],"source":["class CustomCLIPCoCoOp(nn.Module):\n","    def __init__(self, cfg, model):\n","        super().__init__()\n","        self.prompt_learner = CoCoOp(cfg, model)\n","        self.model = model\n","        \n","        # The freezing part was originally done in the training part, but why not just here since we are not modifying anything of CLIP anyway?\n","        for _,param in self.model.named_parameters():\n","            param.requires_grad = False\n","    \n","    # note that this is nearly identical to the model.encode_text() function from CLIP\n","    # the only difference is that we already have prompt_embs rather than having to recompute it\n","    def encode_text(self, prompt_embs, tokenized_prompts):\n","        x = prompt_embs + self.model.positional_embedding.type(self.model.dtype)\n","        x = x.permute(1, 0, 2)  # NLD -> LND\n","        x = self.model.transformer(x)\n","        x = x.permute(1, 0, 2)  # LND -> NLD\n","        x = self.model.ln_final(x).type(self.model.dtype)\n","\n","        # x.shape = [batch_size, n_ctx, transformer.width]\n","        # take features from the eot embedding (eot_token is the highest number in each sequence)\n","        x = x[torch.arange(x.shape[0]), tokenized_prompts.argmax(dim=-1)] @ self.model.text_projection\n","        return x\n","    \n","    def forward(self, img_feats):\n","        logit_scale = self.model.logit_scale.exp()\n","        tokenized_prompts = self.prompt_learner.tokenized_prompts\n","        prompt_embs = self.prompt_learner(img_feats)\n","        \n","        # normalize\n","        img_feats = img_feats/img_feats.norm(dim=-1, keepdim=True).type(self.model.dtype)   # It was float16 but model.dtype = float32.\n","        \n","        logits = []\n","        for pts_i, img_i in zip(prompt_embs, img_feats):\n","            txt_feats = self.encode_text(pts_i, tokenized_prompts)\n","            txt_feats = txt_feats / txt_feats.norm(dim=-1, keepdim=True)\n","            l_i = logit_scale * img_i @ txt_feats.t()\n","            logits.append(l_i)\n","\n","        return torch.stack(logits)"]},{"cell_type":"markdown","metadata":{"id":"SjefaHPkVPRS"},"source":["# Train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cpLuXlq_VPRS"},"outputs":[],"source":["def train(model, data, criterion, optimizer, scheduler, n_epoch, n_shots):\n","    for epoch in range(n_epoch):\n","        print(f\"Epoch {epoch+1}/{n_epoch}:\")\n","        print('-' * 20)\n","        running_loss = 0.0\n","        running_corr = 0\n","        \n","        for batch in data:\n","            img_embs, lbls = batch[\"image\"].to(device), batch[\"label\"].to(device)\n","            outputs = model(img_embs)\n","            loss = criterion(outputs, lbls)\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            \n","            _, preds = torch.max(outputs, 1)\n","            running_loss += loss.item()*img_embs.size(0)\n","            running_corr += torch.sum(preds==lbls.data)\n","            \n","        epoch_loss = running_loss / n_shots\n","        epoch_acc = running_corr.double() / n_shots\n","        print('loss: {:.4f}; acc: {:.4f}'.format(epoch_loss, epoch_acc))\n","        scheduler.step()"]},{"cell_type":"markdown","metadata":{"id":"d5pl4ZeeVPRS"},"source":["Init model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"67n60fo0VPRS"},"outputs":[],"source":["model = CustomCLIPCoCoOp(cfg, clip.load(cfg.MODEL.NAME, device='cpu')[0]) if cfg.USE_COCOOP else CustomCLIPCoOp(cfg, clip.load(cfg.MODEL.NAME, device='cpu')[0])\n","model.to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.prompt_learner.parameters(), lr=cfg.OPTIM.LR)\n","scheduler = build_lr_scheduler(optimizer, cfg.OPTIM)"]},{"cell_type":"markdown","metadata":{"id":"GRTljnweVPRT"},"source":["load img embs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F211IuA6VPRT","outputId":"1af2c294-d02b-4a97-eb59-1663fa2b99dd"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([1020, 512]) torch.Size([1020])\n"]},{"data":{"text/plain":["(tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n","          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n","          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n","          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n","          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n","          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n","          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n","          98,  99, 100, 101]),\n"," tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n","         10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n","         10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n","         10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n","         10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n","         10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10]))"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["img_embs, lbls = torch.load(f'checkpoints/{cfg.DATASET.NAME}_image_embs.pt'), torch.load(f'checkpoints/{cfg.DATASET.NAME}_labels.pt')\n","print(img_embs.shape, lbls.shape)\n","torch.unique(lbls, return_counts=True)"]},{"cell_type":"markdown","metadata":{"id":"_rYfnDxVVPRT"},"source":["select few-shot examples"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2-4RS0yrVPRT"},"outputs":[],"source":["# THIS PART SHOULD BE EDITTED TO MATCH THEIR FEW-SHOT EXAMPLES IF PROVIDED\n","img_embs, lbls = img_embs[0:31:10], lbls[0:31:10]\n","\n","# manually build dataloader (since we only have very few shots this will just be one batch)\n","data = [{'image': img_embs, 'label': lbls}]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4VjAtMZUVPRT","outputId":"622be4ec-849f-40f9-8a38-25f04b8f7e3a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/50:\n","--------------------\n","torch.Size([102, 77, 512])\n","torch.Size([102, 77])\n","loss: 3.5033; acc: 0.2500\n","Epoch 2/50:\n","--------------------\n","torch.Size([102, 77, 512])\n","torch.Size([102, 77])\n","loss: 3.4098; acc: 0.2500\n","Epoch 3/50:\n","--------------------\n","torch.Size([102, 77, 512])\n","torch.Size([102, 77])\n","loss: 1.9729; acc: 0.5000\n","Epoch 4/50:\n","--------------------\n","torch.Size([102, 77, 512])\n","torch.Size([102, 77])\n","loss: 1.5523; acc: 0.5000\n","Epoch 5/50:\n","--------------------\n","torch.Size([102, 77, 512])\n","torch.Size([102, 77])\n","loss: 1.1077; acc: 0.7500\n","Epoch 6/50:\n","--------------------\n","torch.Size([102, 77, 512])\n","torch.Size([102, 77])\n","loss: 1.2063; acc: 0.7500\n","Epoch 7/50:\n","--------------------\n","torch.Size([102, 77, 512])\n","torch.Size([102, 77])\n","loss: 0.6031; acc: 0.7500\n","Epoch 8/50:\n","--------------------\n","torch.Size([102, 77, 512])\n","torch.Size([102, 77])\n","loss: 0.4643; acc: 0.7500\n","Epoch 9/50:\n","--------------------\n","torch.Size([102, 77, 512])\n","torch.Size([102, 77])\n","loss: 0.4591; acc: 0.7500\n","Epoch 10/50:\n","--------------------\n","torch.Size([102, 77, 512])\n","torch.Size([102, 77])\n","loss: 0.3270; acc: 0.7500\n","Epoch 11/50:\n","--------------------\n","torch.Size([102, 77, 512])\n","torch.Size([102, 77])\n","loss: 0.2741; acc: 1.0000\n","Epoch 12/50:\n","--------------------\n","torch.Size([102, 77, 512])\n","torch.Size([102, 77])\n","loss: 0.3756; acc: 0.7500\n","Epoch 13/50:\n","--------------------\n","torch.Size([102, 77, 512])\n","torch.Size([102, 77])\n","loss: 0.3467; acc: 0.7500\n","Epoch 14/50:\n","--------------------\n","torch.Size([102, 77, 512])\n","torch.Size([102, 77])\n","loss: 0.4109; acc: 0.7500\n","Epoch 15/50:\n","--------------------\n","torch.Size([102, 77, 512])\n","torch.Size([102, 77])\n","loss: 0.2167; acc: 1.0000\n","Epoch 16/50:\n","--------------------\n","torch.Size([102, 77, 512])\n","torch.Size([102, 77])\n","loss: 0.1566; acc: 1.0000\n","Epoch 17/50:\n","--------------------\n","torch.Size([102, 77, 512])\n","torch.Size([102, 77])\n","loss: 0.1203; acc: 1.0000\n","Epoch 18/50:\n","--------------------\n","torch.Size([102, 77, 512])\n","torch.Size([102, 77])\n","loss: 0.0970; acc: 1.0000\n","Epoch 19/50:\n","--------------------\n","torch.Size([102, 77, 512])\n","torch.Size([102, 77])\n","loss: 0.0810; acc: 1.0000\n","Epoch 20/50:\n","--------------------\n","torch.Size([102, 77, 512])\n","torch.Size([102, 77])\n","loss: 0.0696; acc: 1.0000\n","Epoch 21/50:\n","--------------------\n","torch.Size([102, 77, 512])\n","torch.Size([102, 77])\n","loss: 0.0608; acc: 1.0000\n","Epoch 22/50:\n","--------------------\n","torch.Size([102, 77, 512])\n","torch.Size([102, 77])\n","loss: 0.0540; acc: 1.0000\n","Epoch 23/50:\n","--------------------\n","torch.Size([102, 77, 512])\n","torch.Size([102, 77])\n","loss: 0.0487; acc: 1.0000\n","Epoch 24/50:\n","--------------------\n","torch.Size([102, 77, 512])\n","torch.Size([102, 77])\n","loss: 0.0444; acc: 1.0000\n","Epoch 25/50:\n","--------------------\n","torch.Size([102, 77, 512])\n","torch.Size([102, 77])\n","loss: 0.0410; acc: 1.0000\n","Epoch 26/50:\n","--------------------\n","torch.Size([102, 77, 512])\n","torch.Size([102, 77])\n","loss: 0.0383; acc: 1.0000\n","Epoch 27/50:\n","--------------------\n","torch.Size([102, 77, 512])\n","torch.Size([102, 77])\n","loss: 0.0360; acc: 1.0000\n","Epoch 28/50:\n","--------------------\n","torch.Size([102, 77, 512])\n","torch.Size([102, 77])\n","loss: 0.0342; acc: 1.0000\n","Epoch 29/50:\n","--------------------\n","torch.Size([102, 77, 512])\n","torch.Size([102, 77])\n","loss: 0.0326; acc: 1.0000\n","Epoch 30/50:\n","--------------------\n","torch.Size([102, 77, 512])\n","torch.Size([102, 77])\n","loss: 0.0312; acc: 1.0000\n","Epoch 31/50:\n","--------------------\n","torch.Size([102, 77, 512])\n","torch.Size([102, 77])\n","loss: 0.0301; acc: 1.0000\n","Epoch 32/50:\n","--------------------\n","torch.Size([102, 77, 512])\n","torch.Size([102, 77])\n","loss: 0.0291; acc: 1.0000\n","Epoch 33/50:\n","--------------------\n","torch.Size([102, 77, 512])\n","torch.Size([102, 77])\n","loss: 0.0282; acc: 1.0000\n","Epoch 34/50:\n","--------------------\n","torch.Size([102, 77, 512])\n","torch.Size([102, 77])\n","loss: 0.0275; acc: 1.0000\n","Epoch 35/50:\n","--------------------\n","torch.Size([102, 77, 512])\n","torch.Size([102, 77])\n","loss: 0.0269; acc: 1.0000\n","Epoch 36/50:\n","--------------------\n","torch.Size([102, 77, 512])\n","torch.Size([102, 77])\n","loss: 0.0263; acc: 1.0000\n","Epoch 37/50:\n","--------------------\n","torch.Size([102, 77, 512])\n","torch.Size([102, 77])\n","loss: 0.0259; acc: 1.0000\n","Epoch 38/50:\n","--------------------\n","torch.Size([102, 77, 512])\n","torch.Size([102, 77])\n","loss: 0.0254; acc: 1.0000\n","Epoch 39/50:\n","--------------------\n","torch.Size([102, 77, 512])\n","torch.Size([102, 77])\n","loss: 0.0251; acc: 1.0000\n","Epoch 40/50:\n","--------------------\n","torch.Size([102, 77, 512])\n","torch.Size([102, 77])\n","loss: 0.0248; acc: 1.0000\n","Epoch 41/50:\n","--------------------\n","torch.Size([102, 77, 512])\n","torch.Size([102, 77])\n","loss: 0.0246; acc: 1.0000\n","Epoch 42/50:\n","--------------------\n","torch.Size([102, 77, 512])\n","torch.Size([102, 77])\n","loss: 0.0244; acc: 1.0000\n","Epoch 43/50:\n","--------------------\n","torch.Size([102, 77, 512])\n","torch.Size([102, 77])\n","loss: 0.0242; acc: 1.0000\n","Epoch 44/50:\n","--------------------\n","torch.Size([102, 77, 512])\n","torch.Size([102, 77])\n","loss: 0.0241; acc: 1.0000\n","Epoch 45/50:\n","--------------------\n","torch.Size([102, 77, 512])\n","torch.Size([102, 77])\n","loss: 0.0239; acc: 1.0000\n","Epoch 46/50:\n","--------------------\n","torch.Size([102, 77, 512])\n","torch.Size([102, 77])\n","loss: 0.0239; acc: 1.0000\n","Epoch 47/50:\n","--------------------\n","torch.Size([102, 77, 512])\n","torch.Size([102, 77])\n","loss: 0.0238; acc: 1.0000\n","Epoch 48/50:\n","--------------------\n","torch.Size([102, 77, 512])\n","torch.Size([102, 77])\n","loss: 0.0238; acc: 1.0000\n","Epoch 49/50:\n","--------------------\n","torch.Size([102, 77, 512])\n","torch.Size([102, 77])\n","loss: 0.0237; acc: 1.0000\n","Epoch 50/50:\n","--------------------\n","torch.Size([102, 77, 512])\n","torch.Size([102, 77])\n","loss: 0.0237; acc: 1.0000\n"]}],"source":["train(model, data, criterion, optimizer, scheduler, n_epoch=cfg.OPTIM.MAX_EPOCH, n_shots=4)"]}],"metadata":{"accelerator":"TPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":0}